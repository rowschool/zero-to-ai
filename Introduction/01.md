# Day 1

In today's lesson:
- calculate and apply regression analysis to datasets
- understand the difference between linear and non-linear data
- understand how to quantify and validate linear models
- understand how to apply scaling and normalization as part of the data processing step in machine learning

We'll go over:
- scikit-learn
- regression analysis

1. Create a model.
2. Fit (train) the model to the data.
3. Use the model to make predictions.

There is usually no single "right" algorithm - instead, experimentation and validation is key.

## Instructor Do: Univariate Linear Regression

Linear regression is a fundamental algorithm in machine learning.
It is used as a building block for other ML models.
It is easy to understand, calculate and interpret.
It is fast!
It is often good enough. Don't over-engineer your solution.
If your data is linear, then use a linear model.

### What is linear data?

```
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.datasets import make_regression

X, y = make_regression(n_samples=20, n_features=1, random_state=0, noise=4, bias=100.0)
plt.scatter(X, y)

https://imgur.com/qfkIu05
```

The response or output is directly proportional to the input.
We can see from the data that we have a linear trend in our model.
We can use Linear Regression to fit a line through the data.

https://imgur.com/4TA4ryl

Using a trained model allows us to make predictions of the output value (Home Selling Price) given a new input (Number of Bathrooms).

https://imgur.com/XYacgqR

If we get a new house on the market:

https://imgur.com/m8Q8KKc

We can use our linear model to predict the price of that house.

https://imgur.com/1iIBMeM

### What about non-linear data?

```
from sklearn.datasets import make_s_curve

data, color = make_s_curve(100, random_state=0)
plt.scatter(data[:,0], color)
```

https://imgur.com/EHn3DsG

### Linear Regression

A regression line is simply calculating a line that best fits the data. This is typically done through the least squares method where the line is chosen to have the smallest overall distance to the points.

y = &theta;<sub>0</sub> + &theta;<sub>1</sub>x

- y is the output response
- x is the input feature
- &theta;<sub>0</sub> is the y-axis intercept
- &theta;<sub>1</sub> is the weight coefficient (slope)

### Sklearn

The Sklearn library provides us with a Linear Regression model that will fit a line to our data. Sklearn follows a consistent API where you define a model object, fit the model to the data, and then make predictions with the model.

```
from sklearn.linear_model import LinearRegression

# create the model
model = LinearRegression()

# fit the model to our data
model.fit(X, y)

# the _ suffix means the attribute is available after the model is fit to the data (trained)
model.coef_ # weight coefficients
model.intercept_ # y-axis intercept

# Our linear model is now of the form:
# y = 101.90 + 12.44x

# we can use our model to make predictions
predictions = model.predict(X)

y[0] # true output
predictions[0] # predicted output
predictions[0] - y[0] # prediction error

pd.DataFrame({"Predicted": predictions, "Actual": y, "Error": predictions - y})[["Predicted", "Actual", "Error"]]

| --- | Predicted | Actual | Error |
...
```

We can calculate the output response for the minimum and maximum input values.
Note: This is useful later when we want to plot the fit line.

x_min = X.min()
x_max = X.max()

y_min_actual = y.min() # actual min value
y_max_actual = y.max() # actual max value

y_min = 101.90 + 12.44 * x_min # calculated min value
y_max = 101.90 + 12.44 * x_max # calculated max value

We can also use the predict function to calculate predicted values.

y_min_predicted = model.predict(x_min) # predicted min value
y_max_predicted = model.predict(x_max) # predicted max value

We can show the model fit by plotting the predicted values against the original data.

plt.scatter(X, y, c="blue")
plt.plot([x_min, x_max], [y_min, y_max], c="red")

https://imgur.com/aEAS0zs

## Students Do: Univariate Linear Regression

### Dataset:  lsd.csv

Source: Wagner, Agahajanian, and Bing (1968). Correlation of Performance
Test Scores with Tissue Concentration of Lysergic Acid Diethylamide in
Human Subjects. Clinical Pharmacology and Therapeutics, Vol.9 pp635-638.

Description: Group of volunteers was given LSD, their mean scores on
math exam and tissue concentrations of LSD were obtained at n=7 time points.

Variables/Columns

TC: Tissue Concentration   1-4
SCORE: Math Score          8-12

```
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Read the csv file into a pandas dataframe
lsd = pd.read_csv("../Resources/lsd.csv")
lsd.head()

# Assign the data to X and y
# Note: Sklearn requires a 7 x 1 array of values
# So we use reshape(-1, 1) to create this
# This is only necessary for a 1D array
X = lsd.tc.values.reshape(-1, 1)
y = lsd.score.values.reshape(-1, 1)

print("Shape: ", X.shape, y.shape)
X

# Plot the data
# YOUR CODE HERE
plt.scatter(X, y)

# Create the model and fit the model to the data
from sklearn.linear_model import LinearRegression
# YOUR CODE HERE
model = LinearRegression()

# Fit the model to the data
# Note: This is the training step where you fit the line to the data.
# YOUR CODE HERE
model.fit(X, y)

# Print the coefficient and the intercept for the model
# YOUR CODE HERE
model.coef_
model.intercept_

# Note: We have to transform our min and max values so they are in the format: array([[ 1.17 ]])
# This is the required format for model.predict()
x_min = np.array([[X.min()]])
x_max = np.array([[X.max()]])

# Calculate the y_min and y_max using model.predict and x_min and x_max
# YOUR CODE HERE
y_min = model.predict(x_min)
y_max = model.predict(x_max)

# Plot X and y using plt.scatter
# Plot the model fit line using [x_min[0], x_max[0], y_min[0], y_max[0]]
# YOUR CODE HERE
plt.scatter(X, y, c="blue")
plt.plot([x_min[0], x_max[0]], [y_min[0], y_max[0]], c="red")
```

Once you master the Model, Fit, Predict steps, it's easier to switch to other ML models.

## Instructor Do: Quantifying Regression

We're going to show the importance of validation by splitting data into training and testing sets.

We'll slice 80% of our data to use as part of our training set.
We'll use the other 20% to predict values for our testing set.

1. Fit (Train) the model.
We can predict our output by using the training data to fit the model to the data.
This is the training step where we predict our output using a given set of features.
Once the model is trained, we can use it to make predictions.

2. Test the trained model.
We can use the test data to make new predictions.
We can compare this actual output to our predictions made by the model.
We can calculate an accuracy score for the model.

3. We can use the model for predicting if the R^2 or Mean Squared Error are within a certain range of desired values.

```
# import dependencies
from sklearn.datasets import make_regression
from sklearn.linear_model import LinearRegression

# generate some data
X, y = make_regression(n_samples=20, n_features=1, random_state=0, noise=4, bias=100.0)

# create a linear model
model = LinearRegression()

# fit (train) the model to the data
model.fit(X, y)
```

# Quantifying our model (MSE or R^2 score)

There are a variety of ways to quantify our model.

from sklearn.metrics import mean_squared_error, r2_score

```
# use our model to predict a value
predicted = model.predict(X)

# score the prediction with MSE and R^2
mse = mean_squared_error(y, predicted) # 11.93
r2 = r2_score(y, predicted) # 0.90

A "good" MSE score will be close to zero while a "good" R2 score will be close to 1.
R2 is the default scoring for many of the Sklearn models.

# overall score for the model
model.score(X, y) # 0.90
```

# Validation

We also want to understand how well our model performs on new data.
One approach for this is to split your data into a training and testing dataset.
You fit (train) the model using training data, and score and validate your model using the testing data.
This train/test splitting is so common that Sklearn provides a mechanism for doing this.

# Testing and Training Data

In order to quantify our model against new input values, we often split the data into training and testing data.
The model is then fit to the training data and scored by test data.
Sklearn pre-processing provides a library for automatically splitting up the data into training and testing.

```
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# train the model using the training data
model.fit(X_train, y_train)

# score the model using the unseen testing data
model.score(X_test, y_test) # 0.92
```

## I Do: Quantifying Regression

First we visualized our data. Now we need a quantification metric to judge our model's strength - R^2 and MSE are two such metrics.

```
from sklearn.datasets import make_regression
from sklearn.linear_model import LinearRegression

# generate data
X, y = make_regression(n_samples=20, n_features=1, random_state=0, noise=4, bias=100.0)

model = LinearRegression()
model.fit(X, y) # fit/train

# quantifying the model with R^2 and MSE
from sklearn.metrics import mean_squared_error, r2_score

predicted = model.predict(X)

mse = mean_squared_error(y, predicted)
r2 = r2_score(y, predicted)

model.score(X, y)
```

A good MSE score will be close to 0.
A good R^2 score is closer to 1.

R^2 is the default scoring system for many sklearn models.

# NOTE: The above is duplicated.

## Students Do - Brains!

In this activity, students calculate a regression line to predict head size vs. brain weight.

Dataset: brain.csv
Source: R.J. Gladstone (1905). "A Study of the Relations of the Brain to to the Size of the Head", Biometrika, Vol. 4, pp105-123

Description: Brain weight (grams) and head size (cubic cm) for 237 adults classified by gender and age group.

Variables/Columns GENDER: Gender /1=Male, 2=Female/ AGE: Age Range /1=20-46, 2=46+/ SIZE: Head size (cm^3) 21-24 WEIGHT: Brain weight (grams) 29-32

```
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Read the csv file into a pandas DataFrame

brains = pd.read_csv('../Resources/brain.csv')
# brains.head()

# Assign the data to X and y
# Note: Sklearn requires a 7 x 1 array of values
# so we use reshape to create this

X = brains["weight"].values.reshape(-1, 1)
y = brains["size"].values.reshape(-1, 1)

print("Shape: ", X.shape, y.shape) # Shape:  (237, 1) (237, 1)

# Plot the data to see if a linear trend exists

### BEGIN SOLUTION
plt.scatter(X, y)
plt.xlabel("Brain Weight")
plt.ylabel("Head Size")
### END SOLUTION

https://imgur.com/brkaB7G

# Use sklearn's `train_test_split` to split the data into training and testing
from sklearn.model_selection import train_test_split

### BEGIN SOLUTION
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
### END SOLUTION

# Create the model
### BEGIN SOLUTION
from sklearn.linear_model import LinearRegression
model = LinearRegression()
### END SOLUTION

# Fit the model to the trainig data.

### BEGIN SOLUTION
model.fit(X_train, y_train)
### END SOLUTION

# Calculate the mean_squared_error and the r-squared value
# for the testing data
from sklearn.metrics import mean_squared_error, r2_score

### BEGIN SOLUTION

# Use our model to make predictions
predicted = model.predict(X_test)

# Score the predictions with mse and r2
mse = mean_squared_error(y_test, predicted)
r2 = r2_score(y_test, predicted)

print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared (R2 ): {r2}")

### END SOLUTION

# Call the `score` method on the model to show the r2 score
### BEGIN SOLUTION
model.score(X_test, y_test)
### END SOLUTION
```

Remind students that the data must be reshaped because sklearn expects the data in a particular format.

Ask the students why the MSE score is so large. Explain that this is because MSE is not upper bounded. Optionally, slack out the formula for MSE (https://en.wikipedia.org/wiki/Mean_squared_error).

Intuitively, the model should perform better on data that it has seen before versus data it has not seen. This means that the model should perform better on the training data than the testing data by definition.

Note that r2_score and model.score produce the same R2 score.

# Instructor Do: Multiple Linear Regression

[Interpreting Residual Plots](http://docs.statwing.com/interpreting-residual-plots-to-improve-your-regression/)

[Linear Regression](http://cs229.stanford.edu/notes/cs229-notes1.pdf)

Multiple Linear Regression simply means that you have more than one feature variable.

Explain that multiple linear regression is linear regression using multiple input features. Use the house price example as an analogy. With univariate linear regression, the trend might predict the price of a home dependent on one variable: square feet. Multiple linear regression allows multiple inputs such as the number of bedrooms, number of bathrooms, as well as square feet.

For the Housing Price example, you may have features like this:

$Y_i$ = $Bias_0$ + $Weight_1$ sq_feet + $Weight_2$ num_bedrooms + $Weight_3$ num_bathrooms

Note: The weights are how important each feature is to the equation. This is the part that the algorithm has to learn.

The generic formula is:

$Y_i = Bias_0 + Weight_1 Feature_1 + Weight_2 Feature_2 + \ldots + Weight_p Feature_p$

The equation is often written as:

$Y_i = \theta_0 + \theta_1 X_{i1} + \theta_2 X_{i2} + \ldots + \theta_p X_{ip}$

With 3 or more dimensions, it becomes harder to visualize the linear trends in our data
We can still visualize 3 features as a 3D plot, but what about n-dimensions? This becomes very difficult for the human brain to visualize.
We could pick just one feature from X to fit our model, but what we really want it to find a line that best fits the data in n-dimensional space. To achieve this, Linear Regression can be solved using the analytical approach called [Ordinary Least Squares](https://en.wikipedia.org/wiki/Ordinary_least_squares) or a computational approach [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent) for estimating the parameters. Note that there are [tradeoffs](https://stats.stackexchange.com/questions/23128/solving-for-regression-parameters-in-closed-form-vs-gradient-descent) between using either approach. The Linear Regression model in Sklearn uses the Ordinary Least Squares method.
Luckily, we can just supply our n-dimensional features and sklearn will fit the model using all of our features.

```
# Generate a linear dataset with 3 features
from sklearn.datasets import make_regression

n_features = 3
X, y = make_regression(n_samples=30, n_features=n_features,
                       n_informative=n_features, random_state=42,
                       noise=0.5, bias=100.0)
print(X.shape) # (30, 3)

from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure(1, figsize=(5, 5))
axes = Axes3D(fig, elev=20, azim=45)
axes.scatter(X[:,0], X[:,1], X[:,2], c=y, cmap=plt.cm.spectral)
plt.show()

https://imgur.com/iqjiFhY

from sklearn.linear_model import LinearRegression
model = LinearRegression()

# Fitting our model with all of our features in X
model.fit(X, y)

score = model.score(X, y)
print(f"R2 Score: {score}")

# residuals
predictions = model.predict(X)
# Plot Residuals
plt.scatter(predictions, predictions - y)
plt.hlines(y=0, xmin=predictions.min(), xmax=predictions.max())
plt.show()

Because we can't easily plot our line in 3D space, we can use a residual plot to check our predictions.

Residuals are the difference between the true values of y and the predicted values of y.

We want our predictions to be close to zero on the y-axis in this plot.

https://imgur.com/tu4MUTa

# Students Do: Beer Foam

## Dataset: beer_foam.csv

Source: J.J. Hackbarth (2006). "Multivariate Analyses of Beer Foam Stand,"
Journal of the Institute of Brewing, Vol. 112, #1, pp. 17-24

Description: Measurements of wet foam height and beer height at various
time points for Shiner Bock at 20C. Author fits exponential decay model:
H(t) = H(0)*exp(-lambda*t)

Variables/Columns
TIME: Time from pour (seconds)  4-8
FOAM: Wet foam height (cm)  10-16
BEER: Beer height (cm)    18-24

**Hypothesis**: Can we predict the time from pour using the measurements of foam height and beer height?

```
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Read the csv file into a pandas DataFrame
foam = pd.read_csv('../Resources/foam.csv')
foam.head()

# Assign the data to X and y
X = foam[["foam", "beer"]]
y = foam["time"].values.reshape(-1, 1)
print(X.shape, y.shape)

# Use train_test_split to create training and testing data

### BEGIN SOLUTION
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

### END SOLUTION

# Create the model using LinearRegression

### BEGIN SOLUTION
from sklearn.linear_model import LinearRegression
model = LinearRegression()
### END SOLUTION

# Fit the model to the training data and calculate the scores for the training and testing data

### BEGIN SOLUTION
model.fit(X_train, y_train)
training_score = model.score(X_train, y_train)
testing_score = model.score(X_test, y_test)

### END SOLUTION

print(f"Training Score: {training_score}")
print(f"Testing Score: {testing_score}")

# Plot the Residuals for the Training and Testing data

### BEGIN SOLUTION
plt.scatter(model.predict(X_train), model.predict(X_train) - y_train, c="blue", label="Training Data")
plt.scatter(model.predict(X_test), model.predict(X_test) - y_test, c="orange", label="Testing Data")
plt.legend()
plt.hlines(y=0, xmin=y.min(), xmax=y.max())
plt.title("Residual Plot")
### END SOLUTION

https://imgur.com/EbUtz5O
```

Explain that we are now using 2 features for our X data, foam and beer. Using more than one feature (independent variable) is considered multiple regression.

Show that our api is the same. That is, we still use the model, fit, predict interface with sklearn. Only the dimensionality of the data has changed. Point out that we do not have to reshape our X data because it is already in the format that sklearn expects. Only 1 dimensional input vectors have to be reshaped.

Explain that we will often see a higher r2 score using multiple regression over simple (1 independent variable) regression. This is because we are using more data to make our predictions.

Show the residual plot for this model using both training and testing data. We do have outliers in this plot which may indicate that our model would not perform as expected. It's hard to say without testing with more data points.

# Instructor Do: Data Preprocessing

## Dataset:  brain_categorical.csv

Source: R.J. Gladstone (1905). "A Study of the Relations of the Brain to
to the Size of the Head", Biometrika, Vol. 4, pp105-123

Description: Brain weight (grams) and head size (cubic cm) for 237
adults classified by gender and age group.

Variables/Columns
GENDER: Gender  Male or Female
AGE: Age Range  20-46 or 46+
SIZE: Head size (cm^3)  21-24
WEIGHT: Brain weight (grams)  29-32

```
import warnings
warnings.simplefilter('ignore')

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Read the csv file into a pandas DataFrame
brain = pd.read_csv('../Resources/brain_categorical.csv')
brain.head()

X = brain[["gender", "age", "size"]]
y = brain["weight"].values.reshape(-1, 1)
print(X.shape, y.shape) # (237, 3) (237, 1)
```

### Working with Categorical Data

Machine learning algorithms work with numerical data. We have to convert our strings into meaningful numbers. We often use Integer, One-hot, or Binary encoding.

Sklearn provides a preprocessing library for all these standard preprocessing techniques.

Pandas also provides a get_dummies method that is useful to generate binary encoded data from a Data Frame.

### Dummy Encoding

Dummy Encoding transforms each categorical feature into new columns with a 1 (True) or 0 (False) encoding to represent if that categorical label was present or not in the original row.

Pandas provides a shortcut to create Binary Encoded data.

Recall the brain dataset from earlier. X is defined by gender, age, and size values.

```
data = X.copy()

data_binary_encoded = pd.get_dummies(data, columns=["gender"])
data_binary_encoded.head()
```

We can encode multiple columns using `get_dummies`.

```
data = X.copy()

data_binary_encoded = pd.get_dummies(data)
data_binary_encoded.head()
```

## Scaling and Normalization

The final step that we need to perform is scaling and normalization. Many algorithms will perform better with a normalized or scaled dataset.

The primary motivation for scaling is to shift all features to the same numeric scale so that large numerical values do not bias the error calculations during the training cycles.

You may not see a differences with the Sklearn LinearRegression model, but other models that use gradient descent need normalization to help the algorithms converge to a local optima.

Sklearn provides a variety of scaling and normalization options. The two most common are `minmax` and `StandardScaler`.

`MinMax` is where values are scaled from 0 to 1.

Use `StandardScaler` when you don't know anything about your data.

The only time that you may not want to scale is if the magnitudes of your input features have significance that needs to be preserved (eg., pixel values in the MNIST handwriting recognition dataset).

The first step is to split your data into Training and Testing groups using `train_test_split`.

```
from sklearn.model_selection import train_test_split

X = pd.get_dummies(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
X_train.head()
```

### StandardScaler

Now, we fit our StandardScaler model to our training data. We can apply this StandardScaler model to any future data.

Note that we use this fit/transform approach so that we isolate our testing data from the training data that we use to fit our model. Otherwise, we might bias our model to the testing data.

```
from sklearn.preprocessing import StandardScaler

X_scaler = StandardScaler.fit(X_train)

y_scaler = StandardScaler.fit(y_train)

X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

y_train_scaled = y_scaler.transform(y_train)
y_test_scaled = y_scaler.transform(y_test)
```

StandardScaler applies a Gaussian distribution to our data where the mean is 0 and the standard deviation is 1. We can see the difference in the following plots.

```
fig1 = plt.figure(figsize=(12, 6))
axes1 = fig1.add_subplot(1, 2, 1)
axes2 = fig1.add_subplot(1, 2, 2)

axes1.set_title("Original Data")
axes2.set_title("Scaled Data")

maxx = X_train["size"].max()
maxy = y_train.max()
axes1.set_xlim(-maxx + 1, maxx + 1)
axes1.set_ylim(-maxy + 1, maxy + 1)

axes2.set_xlim(-2, 2)
axes2.set_ylim(-2, 2)

def set_axes(ax):
    ax.spines['left'].set_position('center')
    ax.spines['right'].set_color('none')
    ax.spines['bottom'].set_position('center')
    ax.spines['top'].set_color('none')
    ax.xaxis.set_ticks_position('bottom')
    ax.yaxis.set_ticks_position('left')

set_axes(axes1)
set_axes(axes2)

axes1.scatter(X_train["size"], y_train)
axes2.scatter(X_train_scaled[:,0], y_train_scaled[:])

plt.show()
```

### Putting It All Together

```
# convert categorical data to numbers using Integer or Binary Encoding
X = pd.get_dummies(brain[["size", "gender", "age"]])
y = brain["weight"].values.reshape(-1, 1)
X.head()

# split data into training and testing data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# scale or normalize your data
# use StandardScaler if you don't know anything about your data
from sklearn.preprocessing import StandardScaler
X_scaler = StandardScaler().fit(X_train)
y_scaler = StandardScaler().fit(y_train)

X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)
y_train_scaled = y_scaler.transform(y_train)
y_test_scaled = y_scaler.transform(y_test)

# fit the model to the scaled training data and make predictions using the scaled test data
# Plot the results
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train_scaled, y_train_scaled)
plt.scatter(model.predict(X_train_scaled), model.predict(X_train_scaled) - y_train_scaled, c="blue", label="Training Data")
plt.scatter(model.predict(X_test_scaled), model.predict(X_test_scaled) - y_test_scaled, c="orange", label="Testing Data")
plt.legend()
plt.hlines(y=0, xmin=y_test_scaled.min(), xmax=y_test_scaled.max())
plt.title("Residual Plot")
plt.show()

# quantify your model using the scaled data
from sklearn.metrics import mean_squared_error

predictions = model.predict(X_test_scaled)
MSE = mean_squared_error(y_test_scaled, predictions)
r2 = model.score(X_test_scaled, y_test_scaled)

print(f"MSE: {MSE}, R2: {r2}") # MSE: 0.3230551580991294, R2: 0.6804912531419804
```

# Students Do - Respiratory Disease

## Childhood Respiratory Disease
Keywords: polynomial regression, multiple regression.

## Description
FEV (forced expiratory volume) is an index of pulmonary function that measures the volume of air expelled after one second of constant effort. The data contains determinations of FEV on 654 children ages 6-22 who were seen in the Childhood Respiratory Desease Study in 1980 in East Boston, Massachusetts. The data are part of a larger study to follow the change in pulmonary function over time in children.

ID - ID number Age - years FEV - litres Height - inches Sex - Male or Female Smoker - Non = nonsmoker, Current = current smoker

## Source
Tager, I. B., Weiss, S. T., Rosner, B., and Speizer, F. E. (1979). Effect of parental cigarette smoking on pulmonary function in children. American Journal of Epidemiology, 110, 15-26. Rosner, B. (1990). Fundamentals of Biostatistics, 3rd Edition. PWS-Kent, Boston, Massachusetts.

```
# Read the csv file into a pandas DataFrame
smoking = pd.read_csv('../Resources/smoking.csv')
smoking.head()

# Use Pandas get_dummies to convert categorical data
### BEGIN SOLUTION
smoking = pd.get_dummies(smoking)
smoking.head()
### END SOLUTION

# Assign X (data) and y (target)
### BEGIN SOLUTION
X = smoking[['Id', 'Age', 'Height', 'Sex_Female', 'Sex_Male', 'Smoker_Current', 'Smoker_Non']]
y = smoking["FEV"].values.reshape(-1, 1)
print(X.shape, y.shape)
### END SOLUTION

Our dataset has categorical values for the columns `sex` and `smoker`. We need to use the Pandas function `get_dummies` to convert these to binary values.

`get_dummies` will automatically create new columns for each category.

# Split the data into training and testing
### BEGIN SOLUTION
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
### END SOLUTION

We need to fit our scaler model to the training data only.
We do not use testing data because we don't want to bias our scaler with the testing data.
We can then apply the scaler model to our training and testing data.

# Create a StandardScaler model and fit it to the training data
from sklearn.preprocessing import StandardScaler

### BEGIN SOLUTION
X_scaler = StandardScaler().fit(X_train)
y_scaler = StandardScaler().fit(y_train)
### END SOLUTION

# Transform the training and testing data using the X_scaler and y_scaler models
### BEGIN SOLUTION
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)
y_train_scaled = y_scaler.transform(y_train)
y_test_scaled = y_scaler.transform(y_test)
### END SOLUTION

# Create a LinearRegression model and fit it to the scaled training data
### BEGIN SOLUTION
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train_scaled, y_train_scaled)
### END SOLUTION

# Make predictions using the X_test_scaled data
# Plot y_test_scaled vs y_test_scaled
# Scatter plot y_test_scaled vs predictions
### BEGIN SOLUTION
predictions = model.predict(X_test_scaled)
model.fit(X_train_scaled, y_train_scaled)
plt.scatter(model.predict(X_train_scaled), model.predict(X_train_scaled) - y_train_scaled, c="blue", label="Training Data")
plt.scatter(model.predict(X_test_scaled), model.predict(X_test_scaled) - y_test_scaled, c="orange", label="Testing Data")
plt.legend()
plt.hlines(y=0, xmin=y_test_scaled.min(), xmax=y_test_scaled.max())
plt.title("Residual Plot")
plt.show()
### END SOLUTION

# Used X_test_scaled, y_test_scaled, and model.predict(X_test_scaled) to calculate MSE and R2
### BEGIN SOLUTION
from sklearn.metrics import mean_squared_error

MSE = mean_squared_error(y_test_scaled, predictions)
r2 = model.score(X_test_scaled, y_test_scaled)
### END SOLUTION

print(f"MSE: {MSE}, R2: {r2}")

We haven't covered Lasso, Ridge, and ElasticNet yet, but these algorithms follow the same
Model, Fit, Predict pattern (interface, in programming speak) consistent with Linear Regression.

# LASSO model
# Note: Use an alpha of .01 when creating the model for this activity
from sklearn.linear_model import Lasso

### BEGIN SOLUTION
lasso = Lasso(alpha=.01).fit(X_train_scaled, y_train_scaled)

predictions = lasso.predict(X_test_scaled)

MSE = mean_squared_error(y_test_scaled, predictions)
r2 = lasso.score(X_test_scaled, y_test_scaled)
### END SOLUTION

print(f"MSE: {MSE}, R2: {r2}")

# Ridge model
# Note: Use an alpha of .01 when creating the model for this activity
from sklearn.linear_model import Ridge

### BEGIN SOLUTION
ridge = Ridge(alpha=.01).fit(X_train_scaled, y_train_scaled)

predictions = ridge.predict(X_test_scaled)

MSE = mean_squared_error(y_test_scaled, predictions)
r2 = ridge.score(X_test_scaled, y_test_scaled)
### END SOLUTION

print(f"MSE: {MSE}, R2: {r2}")

# ElasticNet model
# Note: Use an alpha of .01 when creating the model for this activity
from sklearn.linear_model import ElasticNet

### BEGIN SOLUTION
elasticnet = ElasticNet(alpha=.01).fit(X_train_scaled, y_train_scaled)

predictions = elasticnet.predict(X_test_scaled)

MSE = mean_squared_error(y_test_scaled, predictions)
r2 = elasticnet.score(X_test_scaled, y_test_scaled)
### END SOLUTION

print(f"MSE: {MSE}, R2: {r2}")
```

We find, if we evaluate the performance of these algorithms, that there's no difference.

This will not always be the case, and it's very common to test multiple models to see which has the best performance on a particular dataset. There were no significant advantages to using more complicated algorithms this time, so linear regression is probably still the best choice.
